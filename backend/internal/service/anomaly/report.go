package anomaly

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"kube-node-manager/internal/model"
	"kube-node-manager/pkg/logger"
	"net/http"
	"sync"
	"time"

	"github.com/robfig/cron/v3"
	"gorm.io/gorm"
)

// ReportService æŠ¥å‘Šç”ŸæˆæœåŠ¡
type ReportService struct {
	db         *gorm.DB
	logger     *logger.Logger
	anomalySvc *Service
	scheduler  *cron.Cron
	mu         sync.RWMutex
	jobMap     map[uint]cron.EntryID // config ID -> cron entry ID
	enabled    bool
	ctx        context.Context
	cancel     context.CancelFunc
}

// NewReportService åˆ›å»ºæŠ¥å‘ŠæœåŠ¡å®ä¾‹
func NewReportService(db *gorm.DB, logger *logger.Logger, anomalySvc *Service, enabled bool) *ReportService {
	ctx, cancel := context.WithCancel(context.Background())
	return &ReportService{
		db:         db,
		logger:     logger,
		anomalySvc: anomalySvc,
		scheduler:  cron.New(), // ä½¿ç”¨æ ‡å‡†çš„ 5 å­—æ®µ Cron è¡¨è¾¾å¼ï¼ˆåˆ† æ—¶ æ—¥ æœˆ å‘¨ï¼‰
		jobMap:     make(map[uint]cron.EntryID),
		enabled:    enabled,
		ctx:        ctx,
		cancel:     cancel,
	}
}

// StartScheduler å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
func (rs *ReportService) StartScheduler() {
	if !rs.enabled {
		rs.logger.Info("Anomaly report scheduler is disabled")
		return
	}

	rs.logger.Info("Starting anomaly report scheduler...")

	// ä»æ•°æ®åº“åŠ è½½æ‰€æœ‰å¯ç”¨çš„æŠ¥å‘Šé…ç½®
	if err := rs.SyncSchedulerJobs(); err != nil {
		rs.logger.Errorf("Failed to sync scheduler jobs: %v", err)
	}

	// å¯åŠ¨è°ƒåº¦å™¨
	rs.scheduler.Start()
	rs.logger.Info("Anomaly report scheduler started successfully")
}

// StopScheduler åœæ­¢è°ƒåº¦å™¨
func (rs *ReportService) StopScheduler() {
	if !rs.enabled {
		return
	}

	rs.logger.Info("Stopping anomaly report scheduler...")
	rs.cancel()

	// åœæ­¢è°ƒåº¦å™¨å¹¶ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
	ctx := rs.scheduler.Stop()
	<-ctx.Done()

	rs.logger.Info("Anomaly report scheduler stopped successfully")
}

// SyncSchedulerJobs åŒæ­¥æ•°æ®åº“é…ç½®åˆ°è°ƒåº¦å™¨
func (rs *ReportService) SyncSchedulerJobs() error {
	rs.mu.Lock()
	defer rs.mu.Unlock()

	// æ¸…é™¤æ‰€æœ‰ç°æœ‰ä»»åŠ¡
	for _, entryID := range rs.jobMap {
		rs.scheduler.Remove(entryID)
	}
	rs.jobMap = make(map[uint]cron.EntryID)

	// ä»æ•°æ®åº“åŠ è½½æ‰€æœ‰å¯ç”¨çš„é…ç½®
	var configs []model.AnomalyReportConfig
	if err := rs.db.Where("enabled = ?", true).Find(&configs).Error; err != nil {
		return fmt.Errorf("failed to load report configs: %w", err)
	}

	// ä¸ºæ¯ä¸ªé…ç½®æ³¨å†Œè°ƒåº¦ä»»åŠ¡
	for _, config := range configs {
		if err := rs.addSchedulerJob(config); err != nil {
			rs.logger.Errorf("Failed to add scheduler job for config %d: %v", config.ID, err)
			continue
		}
	}

	rs.logger.Infof("Synced %d report scheduler jobs", len(configs))
	return nil
}

// addSchedulerJob æ·»åŠ å•ä¸ªè°ƒåº¦ä»»åŠ¡
func (rs *ReportService) addSchedulerJob(config model.AnomalyReportConfig) error {
	if config.Schedule == "" {
		return fmt.Errorf("empty schedule for config %d", config.ID)
	}

	// åˆ›å»ºä»»åŠ¡å‡½æ•°
	job := func() {
		rs.logger.Infof("Executing scheduled report: %s (ID: %d)", config.ReportName, config.ID)
		if err := rs.ExecuteReport(config.ID); err != nil {
			rs.logger.Errorf("Failed to execute report %d: %v", config.ID, err)
		}
	}

	// æ³¨å†Œåˆ°è°ƒåº¦å™¨
	entryID, err := rs.scheduler.AddFunc(config.Schedule, job)
	if err != nil {
		return fmt.Errorf("failed to add cron job: %w", err)
	}

	rs.jobMap[config.ID] = entryID

	// è®¡ç®—ä¸‹æ¬¡æ‰§è¡Œæ—¶é—´
	entry := rs.scheduler.Entry(entryID)
	nextTime := entry.Next

	// æ›´æ–°æ•°æ®åº“ä¸­çš„ä¸‹æ¬¡æ‰§è¡Œæ—¶é—´ï¼ˆä½¿ç”¨ Where å’Œ Updates ç¡®ä¿æ›´æ–°ç”Ÿæ•ˆï¼‰
	if err := rs.db.Model(&model.AnomalyReportConfig{}).
		Where("id = ?", config.ID).
		Update("next_run_time", nextTime).Error; err != nil {
		rs.logger.Warningf("Failed to update next run time for config %d: %v", config.ID, err)
	}

	rs.logger.Infof("Added scheduler job for report '%s' (ID: %d), next run: %v", config.ReportName, config.ID, nextTime)
	return nil
}

// GetReportConfigs è·å–æ‰€æœ‰æŠ¥å‘Šé…ç½®
func (rs *ReportService) GetReportConfigs() ([]model.AnomalyReportConfig, error) {
	configs := make([]model.AnomalyReportConfig, 0) // åˆå§‹åŒ–ä¸ºç©ºæ•°ç»„è€Œä¸æ˜¯ nil
	if err := rs.db.Order("id DESC").Find(&configs).Error; err != nil {
		return nil, fmt.Errorf("failed to get report configs: %w", err)
	}
	return configs, nil
}

// GetReportConfig è·å–å•ä¸ªæŠ¥å‘Šé…ç½®
func (rs *ReportService) GetReportConfig(id uint) (*model.AnomalyReportConfig, error) {
	var config model.AnomalyReportConfig
	if err := rs.db.First(&config, id).Error; err != nil {
		return nil, fmt.Errorf("failed to get report config: %w", err)
	}
	return &config, nil
}

// CreateReportConfig åˆ›å»ºæŠ¥å‘Šé…ç½®
func (rs *ReportService) CreateReportConfig(config *model.AnomalyReportConfig) error {
	// éªŒè¯ Cron è¡¨è¾¾å¼
	if config.Schedule != "" {
		if _, err := cron.ParseStandard(config.Schedule); err != nil {
			return fmt.Errorf("invalid cron expression: %w", err)
		}
	}

	// åˆ›å»ºé…ç½®
	if err := rs.db.Create(config).Error; err != nil {
		return fmt.Errorf("failed to create report config: %w", err)
	}

	// å¦‚æœé…ç½®å·²å¯ç”¨ï¼Œæ·»åŠ åˆ°è°ƒåº¦å™¨
	if config.Enabled && rs.enabled {
		rs.mu.Lock()
		if err := rs.addSchedulerJob(*config); err != nil {
			rs.logger.Errorf("Failed to add scheduler job for new config %d: %v", config.ID, err)
		}
		rs.mu.Unlock()
	}

	return nil
}

// UpdateReportConfig æ›´æ–°æŠ¥å‘Šé…ç½®
func (rs *ReportService) UpdateReportConfig(id uint, updates *model.AnomalyReportConfig) error {
	// éªŒè¯ Cron è¡¨è¾¾å¼
	if updates.Schedule != "" {
		if _, err := cron.ParseStandard(updates.Schedule); err != nil {
			return fmt.Errorf("invalid cron expression: %w", err)
		}
	}

	// æ›´æ–°é…ç½®
	if err := rs.db.Model(&model.AnomalyReportConfig{}).Where("id = ?", id).Updates(updates).Error; err != nil {
		return fmt.Errorf("failed to update report config: %w", err)
	}

	// é‡æ–°åŒæ­¥è°ƒåº¦å™¨
	if rs.enabled {
		if err := rs.SyncSchedulerJobs(); err != nil {
			rs.logger.Errorf("Failed to sync scheduler after config update: %v", err)
		}
	}

	return nil
}

// DeleteReportConfig åˆ é™¤æŠ¥å‘Šé…ç½®
func (rs *ReportService) DeleteReportConfig(id uint) error {
	// ä»è°ƒåº¦å™¨ç§»é™¤
	rs.mu.Lock()
	if entryID, exists := rs.jobMap[id]; exists {
		rs.scheduler.Remove(entryID)
		delete(rs.jobMap, id)
	}
	rs.mu.Unlock()

	// ä»æ•°æ®åº“åˆ é™¤
	if err := rs.db.Delete(&model.AnomalyReportConfig{}, id).Error; err != nil {
		return fmt.Errorf("failed to delete report config: %w", err)
	}

	return nil
}

// ValidateCronExpression éªŒè¯ Cron è¡¨è¾¾å¼
func (rs *ReportService) ValidateCronExpression(cronExpr string) error {
	_, err := cron.ParseStandard(cronExpr)
	return err
}

// ExecuteReport æ‰§è¡ŒæŠ¥å‘Šç”Ÿæˆï¼ˆæ‰‹åŠ¨æˆ–å®šæ—¶è§¦å‘ï¼‰
func (rs *ReportService) ExecuteReport(configID uint) error {
	// è·å–é…ç½®
	config, err := rs.GetReportConfig(configID)
	if err != nil {
		return err
	}

	// è®°å½•å¼€å§‹æ—¶é—´
	startTime := time.Now()

	// ç”ŸæˆæŠ¥å‘Šå†…å®¹
	content, err := rs.GenerateReport(config)
	if err != nil {
		return fmt.Errorf("failed to generate report: %w", err)
	}

	// å‘é€æŠ¥å‘Š
	var sendErrors []error

	if config.FeishuEnabled && config.FeishuWebhook != "" {
		if err := rs.SendReportToFeishu(config, content); err != nil {
			rs.logger.Errorf("Failed to send report to Feishu: %v", err)
			sendErrors = append(sendErrors, err)
		}
	}

	if config.EmailEnabled && config.EmailRecipients != "" {
		if err := rs.SendReportToEmail(config, content); err != nil {
			rs.logger.Errorf("Failed to send report to Email: %v", err)
			sendErrors = append(sendErrors, err)
		}
	}

	// æ›´æ–°æœ€åæ‰§è¡Œæ—¶é—´
	now := time.Now()
	if err := rs.db.Model(config).Updates(map[string]interface{}{
		"last_run_time": now,
	}).Error; err != nil {
		rs.logger.Warningf("Failed to update last run time: %v", err)
	}

	rs.logger.Infof("Report '%s' executed successfully in %v", config.ReportName, time.Since(startTime))

	if len(sendErrors) > 0 {
		return fmt.Errorf("report generated but failed to send to some channels: %v", sendErrors)
	}

	return nil
}

// ReportContent æŠ¥å‘Šå†…å®¹ç»“æ„
type ReportContent struct {
	ReportName  string
	PeriodStart time.Time
	PeriodEnd   time.Time
	Clusters    []string
	Summary     ReportSummary
	TrendData   []model.AnomalyStatistics
	TypeStats   []model.AnomalyTypeStatistics
	TopNodes    []TopNodeInfo
	SLAMetrics  *model.SLAMetrics
	MTTRStats   []model.MTTRStatistics
}

// ReportSummary æŠ¥å‘Šæ‘˜è¦
type ReportSummary struct {
	TotalAnomalies    int64
	ActiveAnomalies   int64
	ResolvedAnomalies int64
	AffectedNodes     int64
	TotalClusters     int
}

// TopNodeInfo èŠ‚ç‚¹å¼‚å¸¸ä¿¡æ¯
type TopNodeInfo struct {
	NodeName     string
	ClusterName  string
	AnomalyCount int64
	HealthScore  float64
}

// GenerateReport ç”ŸæˆæŠ¥å‘Šå†…å®¹
func (rs *ReportService) GenerateReport(config *model.AnomalyReportConfig) (*ReportContent, error) {
	// ç¡®å®šæŠ¥å‘Šæ—¶é—´èŒƒå›´
	endTime := time.Now()
	var startTime time.Time

	switch config.Frequency {
	case model.ReportFrequencyDaily:
		startTime = endTime.AddDate(0, 0, -1)
	case model.ReportFrequencyWeekly:
		startTime = endTime.AddDate(0, 0, -7)
	case model.ReportFrequencyMonthly:
		startTime = endTime.AddDate(0, -1, 0)
	default:
		startTime = endTime.AddDate(0, 0, -1)
	}

	// è§£æé›†ç¾¤IDåˆ—è¡¨
	var clusterIDs []uint
	if config.ClusterIDs != "" {
		if err := json.Unmarshal([]byte(config.ClusterIDs), &clusterIDs); err != nil {
			rs.logger.Warningf("Failed to parse cluster IDs: %v", err)
		}
	}

	// æ„å»ºæŠ¥å‘Šå†…å®¹
	content := &ReportContent{
		ReportName:  config.ReportName,
		PeriodStart: startTime,
		PeriodEnd:   endTime,
	}

	// è·å–é›†ç¾¤åˆ—è¡¨
	var clusters []model.Cluster
	query := rs.db.Where("status = ?", model.ClusterStatusActive)
	if len(clusterIDs) > 0 {
		query = query.Where("id IN ?", clusterIDs)
	}
	if err := query.Find(&clusters).Error; err == nil {
		for _, c := range clusters {
			content.Clusters = append(content.Clusters, c.Name)
		}
		content.Summary.TotalClusters = len(clusters)
	}

	// è·å–ç»Ÿè®¡æ‘˜è¦
	var clusterID *uint
	if len(clusterIDs) == 1 {
		clusterID = &clusterIDs[0]
	}

	summary, err := rs.anomalySvc.GetAnomalySummary(clusterID)
	if err == nil {
		if totalCount, ok := summary["total_count"].(int64); ok {
			content.Summary.TotalAnomalies = totalCount
		}
		if activeCount, ok := summary["active_count"].(int64); ok {
			content.Summary.ActiveAnomalies = activeCount
		}
		if resolvedCount, ok := summary["resolved_count"].(int64); ok {
			content.Summary.ResolvedAnomalies = resolvedCount
		}
		if affectedNodes, ok := summary["affected_nodes"].(int64); ok {
			content.Summary.AffectedNodes = affectedNodes
		}
	}

	// è·å–è¶‹åŠ¿æ•°æ®
	trendData, err := rs.anomalySvc.GetStatistics(StatisticsRequest{
		ClusterID: clusterID,
		StartTime: &startTime,
		EndTime:   &endTime,
		Dimension: "day",
	})
	if err == nil {
		content.TrendData = trendData
	}

	// è·å–ç±»å‹ç»Ÿè®¡
	typeStats, err := rs.anomalySvc.GetTypeStatistics(clusterID, &startTime, &endTime)
	if err == nil {
		content.TypeStats = typeStats
	}

	// è·å– Top 10 å¼‚å¸¸èŠ‚ç‚¹
	topNodes, err := rs.anomalySvc.GetTopUnhealthyNodes(clusterID, 10, &startTime, &endTime)
	if err == nil {
		for _, node := range topNodes {
			content.TopNodes = append(content.TopNodes, TopNodeInfo{
				NodeName:     node.NodeName,
				ClusterName:  node.ClusterName,
				AnomalyCount: node.TotalAnomalies,
				HealthScore:  node.HealthScore,
			})
		}
	}

	// è·å– MTTR ç»Ÿè®¡
	entityType := "cluster"
	mttrStats, err := rs.anomalySvc.GetMTTRStatistics(entityType, clusterID, &startTime, &endTime)
	if err == nil {
		content.MTTRStats = mttrStats
	}

	return content, nil
}

// SendReportToFeishu å‘é€æŠ¥å‘Šåˆ°é£ä¹¦
func (rs *ReportService) SendReportToFeishu(config *model.AnomalyReportConfig, content *ReportContent) error {
	rs.logger.Infof("Sending report to Feishu webhook: %s", config.FeishuWebhook)

	// æ„å»ºé£ä¹¦å¡ç‰‡
	card := rs.buildFeishuReportCard(content)

	// å‡†å¤‡ webhook è¯·æ±‚ä½“
	webhookReq := map[string]interface{}{
		"msg_type": "interactive",
		"card":     card,
	}

	jsonData, err := json.Marshal(webhookReq)
	if err != nil {
		return fmt.Errorf("failed to marshal webhook request: %w", err)
	}

	// å‘é€ HTTP POST è¯·æ±‚åˆ° webhook
	client := &http.Client{Timeout: 10 * time.Second}
	resp, err := client.Post(config.FeishuWebhook, "application/json", bytes.NewBuffer(jsonData))
	if err != nil {
		return fmt.Errorf("failed to send webhook request: %w", err)
	}
	defer resp.Body.Close()

	// è¯»å–å“åº”
	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return fmt.Errorf("failed to read webhook response: %w", err)
	}

	// æ£€æŸ¥å“åº”çŠ¶æ€
	if resp.StatusCode != http.StatusOK {
		return fmt.Errorf("webhook returned error status %d: %s", resp.StatusCode, string(body))
	}

	// è§£æå“åº” JSON
	var webhookResp map[string]interface{}
	if err := json.Unmarshal(body, &webhookResp); err != nil {
		rs.logger.Warningf("Failed to parse webhook response: %v", err)
		// ä¸è¿”å›é”™è¯¯ï¼Œå› ä¸ºæ¶ˆæ¯å¯èƒ½å·²å‘é€æˆåŠŸ
		return nil
	}

	// æ£€æŸ¥é£ä¹¦ API è¿”å›ç 
	if code, ok := webhookResp["code"].(float64); ok && code != 0 {
		msg := webhookResp["msg"].(string)
		return fmt.Errorf("feishu webhook error: code=%v, msg=%s", code, msg)
	}

	rs.logger.Infof("Successfully sent report to Feishu webhook")
	return nil
}

// SendReportToEmail å‘é€æŠ¥å‘Šåˆ°é‚®ç®±
func (rs *ReportService) SendReportToEmail(config *model.AnomalyReportConfig, content *ReportContent) error {
	// TODO: å®ç°é‚®ä»¶å‘é€é€»è¾‘
	// è¿™é‡Œéœ€è¦ SMTP é…ç½®å’Œé‚®ä»¶æ¨¡æ¿

	rs.logger.Infof("Sending report to email: %s", config.EmailRecipients)
	// å®ç°ç•¥ï¼Œåç»­æ ¹æ®éœ€è¦å®Œå–„
	return nil
}

// TestReportSend æµ‹è¯•æŠ¥å‘Šå‘é€
func (rs *ReportService) TestReportSend(configID uint) error {
	config, err := rs.GetReportConfig(configID)
	if err != nil {
		return err
	}

	// ç”Ÿæˆæµ‹è¯•æŠ¥å‘Šï¼ˆä½¿ç”¨æœ€è¿‘7å¤©æ•°æ®ï¼‰
	endTime := time.Now()
	startTime := endTime.AddDate(0, 0, -7)

	testContent := &ReportContent{
		ReportName:  config.ReportName + " (æµ‹è¯•)",
		PeriodStart: startTime,
		PeriodEnd:   endTime,
	}

	// å‘é€æµ‹è¯•æŠ¥å‘Š
	if config.FeishuEnabled && config.FeishuWebhook != "" {
		if err := rs.SendReportToFeishu(config, testContent); err != nil {
			return fmt.Errorf("failed to send test report to Feishu: %w", err)
		}
	}

	if config.EmailEnabled && config.EmailRecipients != "" {
		if err := rs.SendReportToEmail(config, testContent); err != nil {
			return fmt.Errorf("failed to send test report to Email: %w", err)
		}
	}

	return nil
}

// buildFeishuReportCard æ„å»ºé£ä¹¦æŠ¥å‘Šå¡ç‰‡
func (rs *ReportService) buildFeishuReportCard(content *ReportContent) map[string]interface{} {
	// æ ¼å¼åŒ–æ—¶é—´èŒƒå›´
	timeRange := fmt.Sprintf("%s ~ %s",
		content.PeriodStart.Format("2006-01-02 15:04"),
		content.PeriodEnd.Format("2006-01-02 15:04"))

	// æ„å»ºå…ƒç´ åˆ—è¡¨
	elements := []interface{}{
		// æŠ¥å‘Šæ—¶é—´èŒƒå›´
		map[string]interface{}{
			"tag": "div",
			"text": map[string]interface{}{
				"content": fmt.Sprintf("**ğŸ“… æŠ¥å‘Šå‘¨æœŸ**: %s", timeRange),
				"tag":     "lark_md",
			},
		},
		// é›†ç¾¤ä¿¡æ¯
		map[string]interface{}{
			"tag": "div",
			"text": map[string]interface{}{
				"content": fmt.Sprintf("**ğŸ¢ ç›‘æ§é›†ç¾¤**: %sï¼ˆå…± %d ä¸ªï¼‰",
					formatClusters(content.Clusters),
					content.Summary.TotalClusters),
				"tag": "lark_md",
			},
		},
		map[string]interface{}{
			"tag": "hr",
		},
		// ç»Ÿè®¡æ‘˜è¦
		map[string]interface{}{
			"tag": "div",
			"text": map[string]interface{}{
				"content": "**ğŸ“Š ç»Ÿè®¡æ‘˜è¦**",
				"tag":     "lark_md",
			},
		},
		map[string]interface{}{
			"tag": "div",
			"fields": []interface{}{
				map[string]interface{}{
					"is_short": true,
					"text": map[string]interface{}{
						"tag":     "lark_md",
						"content": fmt.Sprintf("**æ€»å¼‚å¸¸æ•°**\n%d", content.Summary.TotalAnomalies),
					},
				},
				map[string]interface{}{
					"is_short": true,
					"text": map[string]interface{}{
						"tag":     "lark_md",
						"content": fmt.Sprintf("**æ´»è·ƒå¼‚å¸¸**\nğŸ”´ %d", content.Summary.ActiveAnomalies),
					},
				},
			},
		},
		map[string]interface{}{
			"tag": "div",
			"fields": []interface{}{
				map[string]interface{}{
					"is_short": true,
					"text": map[string]interface{}{
						"tag":     "lark_md",
						"content": fmt.Sprintf("**å·²æ¢å¤**\nâœ… %d", content.Summary.ResolvedAnomalies),
					},
				},
				map[string]interface{}{
					"is_short": true,
					"text": map[string]interface{}{
						"tag":     "lark_md",
						"content": fmt.Sprintf("**å—å½±å“èŠ‚ç‚¹**\nâš ï¸ %d", content.Summary.AffectedNodes),
					},
				},
			},
		},
	}

	// æ·»åŠ å¼‚å¸¸ç±»å‹ç»Ÿè®¡
	if len(content.TypeStats) > 0 {
		elements = append(elements, map[string]interface{}{
			"tag": "hr",
		})
		elements = append(elements, map[string]interface{}{
			"tag": "div",
			"text": map[string]interface{}{
				"content": "**ğŸ” å¼‚å¸¸ç±»å‹åˆ†å¸ƒ**",
				"tag":     "lark_md",
			},
		})

		typeTexts := make([]string, 0, len(content.TypeStats))
		for _, ts := range content.TypeStats {
			icon := getAnomalyTypeIcon(string(ts.AnomalyType))
			typeTexts = append(typeTexts, fmt.Sprintf("â€¢ %s %s: %d æ¬¡", icon, ts.AnomalyType, ts.TotalCount))
		}

		elements = append(elements, map[string]interface{}{
			"tag": "div",
			"text": map[string]interface{}{
				"content": formatList(typeTexts, 5),
				"tag":     "lark_md",
			},
		})
	}

	// æ·»åŠ é—®é¢˜èŠ‚ç‚¹ Top 5
	if len(content.TopNodes) > 0 {
		elements = append(elements, map[string]interface{}{
			"tag": "hr",
		})
		elements = append(elements, map[string]interface{}{
			"tag": "div",
			"text": map[string]interface{}{
				"content": "**âš ï¸ å¼‚å¸¸æœ€å¤šçš„èŠ‚ç‚¹ï¼ˆTop 5ï¼‰**",
				"tag":     "lark_md",
			},
		})

		topCount := 5
		if len(content.TopNodes) < topCount {
			topCount = len(content.TopNodes)
		}

		for i := 0; i < topCount; i++ {
			node := content.TopNodes[i]
			healthIcon := getHealthIcon(node.HealthScore)
			elements = append(elements, map[string]interface{}{
				"tag": "div",
				"text": map[string]interface{}{
					"content": fmt.Sprintf("%d. **%s** (%s)\n   å¼‚å¸¸: %d æ¬¡ | å¥åº·åº¦: %s %.1f%%",
						i+1, node.NodeName, node.ClusterName,
						node.AnomalyCount, healthIcon, node.HealthScore),
					"tag": "lark_md",
				},
			})
		}
	}

	// æ·»åŠ æ³¨é‡Š
	elements = append(elements, map[string]interface{}{
		"tag": "hr",
	})
	elements = append(elements, map[string]interface{}{
		"tag": "note",
		"elements": []interface{}{
			map[string]interface{}{
				"tag":     "plain_text",
				"content": "ğŸ’¡ æŸ¥çœ‹è¯¦ç»†æ•°æ®è¯·è®¿é—® Kube Node Manager æ§åˆ¶å°",
			},
		},
	})

	// æ„å»ºå¡ç‰‡
	card := map[string]interface{}{
		"config": map[string]interface{}{
			"wide_screen_mode": true,
		},
		"header": map[string]interface{}{
			"template": "blue",
			"title": map[string]interface{}{
				"content": fmt.Sprintf("ğŸ“Š %s", content.ReportName),
				"tag":     "plain_text",
			},
		},
		"elements": elements,
	}

	return card
}

// formatClusters æ ¼å¼åŒ–é›†ç¾¤åˆ—è¡¨
func formatClusters(clusters []string) string {
	if len(clusters) == 0 {
		return "æ‰€æœ‰é›†ç¾¤"
	}
	if len(clusters) <= 3 {
		result := ""
		for i, c := range clusters {
			if i > 0 {
				result += ", "
			}
			result += c
		}
		return result
	}
	return fmt.Sprintf("%s ç­‰ %d ä¸ª", clusters[0], len(clusters))
}

// formatList æ ¼å¼åŒ–åˆ—è¡¨ï¼Œæœ€å¤šæ˜¾ç¤º maxItems é¡¹
func formatList(items []string, maxItems int) string {
	if len(items) == 0 {
		return "æ— "
	}

	result := ""
	count := maxItems
	if len(items) < count {
		count = len(items)
	}

	for i := 0; i < count; i++ {
		if i > 0 {
			result += "\n"
		}
		result += items[i]
	}

	if len(items) > maxItems {
		result += fmt.Sprintf("\n... è¿˜æœ‰ %d é¡¹", len(items)-maxItems)
	}

	return result
}

// getAnomalyTypeIcon è·å–å¼‚å¸¸ç±»å‹å¯¹åº”çš„å›¾æ ‡
func getAnomalyTypeIcon(anomalyType string) string {
	icons := map[string]string{
		"NotReady":           "ğŸ”´",
		"DiskPressure":       "ğŸ’¾",
		"MemoryPressure":     "ğŸ§ ",
		"PIDPressure":        "âš™ï¸",
		"NetworkUnavailable": "ğŸŒ",
	}
	if icon, ok := icons[anomalyType]; ok {
		return icon
	}
	return "âš ï¸"
}

// getHealthIcon æ ¹æ®å¥åº·åº¦è·å–å›¾æ ‡
func getHealthIcon(score float64) string {
	if score >= 90 {
		return "ğŸŸ¢"
	} else if score >= 70 {
		return "ğŸŸ¡"
	} else if score >= 50 {
		return "ğŸŸ "
	}
	return "ğŸ”´"
}
