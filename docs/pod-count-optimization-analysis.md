# Pod æ•°é‡ç»Ÿè®¡ä¼˜åŒ–æ–¹æ¡ˆåˆ†æ

**åˆ†ææ—¥æœŸ**: 2025-11-03  
**å½“å‰ç‰ˆæœ¬**: v2.23.1

---

## å½“å‰å®ç°åˆ†æ

### ç°æœ‰æ–¹æ¡ˆï¼šåˆ†é¡µæŸ¥è¯¢ + ç¼“å­˜

```go
// å½“å‰å®ç°ï¼šéå†æ‰€æœ‰ Podï¼ŒæŒ‰èŠ‚ç‚¹ç»Ÿè®¡
func (s *Service) getNodesPodCounts(clusterName string, nodeNames []string) map[string]int {
    // 1. åˆ†é¡µè·å–æ‰€æœ‰ Podï¼ˆæ¯é¡µ 1000 ä¸ªï¼‰
    // 2. éå†æ¯ä¸ª Podï¼Œæ£€æŸ¥ nodeName å’Œ status.phase
    // 3. ç»Ÿè®¡éç»ˆæ­¢çŠ¶æ€çš„ Pod æ•°é‡
    // 4. è¿”å› map[nodeName]count
}
```

### ç°æœ‰æ–¹æ¡ˆçš„é—®é¢˜

| é—®é¢˜ | æè¿° | å½±å“ |
|------|------|------|
| **å…¨é‡æŸ¥è¯¢** | éœ€è¦æŸ¥è¯¢é›†ç¾¤æ‰€æœ‰ Pod | å¤§è§„æ¨¡é›†ç¾¤è€—æ—¶ 30-60 ç§’ |
| **å†—ä½™æ•°æ®** | è·å–å®Œæ•´ Pod å¯¹è±¡ï¼ˆæ¯ä¸ª ~50KBï¼‰ | ç½‘ç»œä¼ è¾“å’Œå†…å­˜å ç”¨å¤§ |
| **é‡å¤ç»Ÿè®¡** | æ¯æ¬¡éƒ½é‡æ–°éå†æ‰€æœ‰ Pod | CPU æ¶ˆè€—é«˜ |
| **æ— å¢é‡æ›´æ–°** | æ— æ³•è·Ÿè¸ª Pod å˜åŒ–ï¼Œåªèƒ½å…¨é‡åˆ·æ–° | ç¼“å­˜è¿‡æœŸåå†æ¬¡å…¨é‡æŸ¥è¯¢ |

---

## ä¼˜åŒ–æ–¹æ¡ˆå¯¹æ¯”

### æ–¹æ¡ˆ 1ï¼šè½»é‡çº§ Pod Informerï¼ˆæ¨è â­â­â­â­â­ï¼‰

#### æ ¸å¿ƒæ€è·¯
ä½¿ç”¨ **è½»é‡çº§ Informer**ï¼Œåªç¼“å­˜ Pod çš„å¿…è¦ä¿¡æ¯ï¼ˆnodeName + status.phaseï¼‰ï¼Œä¸ç¼“å­˜å®Œæ•´ Pod å¯¹è±¡ã€‚

#### å®ç°è®¾è®¡

```go
// PodCountCache è½»é‡çº§ Pod ç»Ÿè®¡ç¼“å­˜
type PodCountCache struct {
    // æ¯ä¸ªèŠ‚ç‚¹çš„ Pod è®¡æ•°: cluster:node -> count
    nodePodCounts sync.Map
    
    // Pod ç´¢å¼•: cluster:podUID -> nodeName
    // ç”¨äºå¤„ç† Pod è¿ç§»ï¼ˆä»èŠ‚ç‚¹ A ç§»åˆ°èŠ‚ç‚¹ Bï¼‰
    podToNode sync.Map
    
    logger *logger.Logger
}

// å®ç° PodEventHandler æ¥å£
func (pc *PodCountCache) OnPodEvent(event PodEvent) {
    switch event.Type {
    case EventTypeAdd:
        // Pod åˆ›å»ºï¼šå¯¹åº”èŠ‚ç‚¹è®¡æ•° +1
        pc.incrementPodCount(event.ClusterName, event.Pod.Spec.NodeName)
        pc.podToNode.Store(makeKey(event.ClusterName, event.Pod.UID), event.Pod.Spec.NodeName)
        
    case EventTypeDelete:
        // Pod åˆ é™¤ï¼šå¯¹åº”èŠ‚ç‚¹è®¡æ•° -1
        if nodeName, ok := pc.podToNode.Load(makeKey(event.ClusterName, event.Pod.UID)); ok {
            pc.decrementPodCount(event.ClusterName, nodeName.(string))
            pc.podToNode.Delete(makeKey(event.ClusterName, event.Pod.UID))
        }
        
    case EventTypeUpdate:
        // Pod æ›´æ–°ï¼šæ£€æŸ¥çŠ¶æ€æˆ–èŠ‚ç‚¹å˜åŒ–
        oldNodeName, _ := pc.podToNode.Load(makeKey(event.ClusterName, event.Pod.UID))
        newNodeName := event.Pod.Spec.NodeName
        
        // å¤„ç† Pod è¿ç§»
        if oldNodeName != nil && oldNodeName.(string) != newNodeName {
            pc.decrementPodCount(event.ClusterName, oldNodeName.(string))
            pc.incrementPodCount(event.ClusterName, newNodeName)
            pc.podToNode.Store(makeKey(event.ClusterName, event.Pod.UID), newNodeName)
        }
        
        // å¤„ç†çŠ¶æ€å˜åŒ–ï¼ˆRunning -> Succeeded/Failedï¼‰
        if isTerminated(event.Pod.Status.Phase) {
            pc.decrementPodCount(event.ClusterName, newNodeName)
        }
    }
}

// è·å–èŠ‚ç‚¹ Pod æ•°é‡ï¼ˆå®æ—¶ï¼ŒO(1) æ—¶é—´å¤æ‚åº¦ï¼‰
func (pc *PodCountCache) GetNodePodCount(cluster, nodeName string) int {
    key := makeKey(cluster, nodeName)
    if count, ok := pc.nodePodCounts.Load(key); ok {
        return count.(int)
    }
    return 0
}
```

#### å¯åŠ¨ Pod Informer

```go
// åœ¨ Informer Service ä¸­æ·»åŠ  Pod Informer
func (s *Service) StartPodInformer(clusterName string, clientset *kubernetes.Clientset) error {
    factory := informers.NewSharedInformerFactory(clientset, 30*time.Minute)
    
    // è·å– PodInformerï¼ˆåªç›‘å¬å¿…è¦å­—æ®µï¼‰
    podInformer := factory.Core().V1().Pods().Informer()
    
    // æ³¨å†Œäº‹ä»¶å¤„ç†å™¨
    podInformer.AddEventHandler(cache.ResourceEventHandlerFuncs{
        AddFunc: func(obj interface{}) {
            pod := obj.(*corev1.Pod)
            s.handlePodAdd(clusterName, pod)
        },
        UpdateFunc: func(oldObj, newObj interface{}) {
            oldPod := oldObj.(*corev1.Pod)
            newPod := newObj.(*corev1.Pod)
            s.handlePodUpdate(clusterName, oldPod, newPod)
        },
        DeleteFunc: func(obj interface{}) {
            pod := obj.(*corev1.Pod)
            s.handlePodDelete(clusterName, pod)
        },
    })
    
    // å¯åŠ¨å¹¶ç­‰å¾…ç¼“å­˜åŒæ­¥
    go factory.Start(stopCh)
    cache.WaitForCacheSync(ctx.Done(), podInformer.HasSynced)
    
    return nil
}
```

#### ä¼˜åŠ¿

| ä¼˜åŠ¿ | è¯´æ˜ | æ€§èƒ½æå‡ |
|------|------|----------|
| âœ… **å®æ—¶ç»Ÿè®¡** | å¢é‡æ›´æ–°ï¼Œæ— éœ€å…¨é‡æŸ¥è¯¢ | å“åº”æ—¶é—´ < 1ms |
| âœ… **å†…å­˜å¯æ§** | åªå­˜å‚¨ UID â†’ nodeName æ˜ å°„ï¼ˆ~100 bytes/podï¼‰ | 10k pods â‰ˆ 1MB |
| âœ… **é›¶ API è°ƒç”¨** | æŸ¥è¯¢ä¸éœ€è¦è®¿é—® K8s API | API å‹åŠ›é™ä½ 100% |
| âœ… **å‡†ç¡®æ€§é«˜** | å®æ—¶è·Ÿè¸ª Pod åˆ›å»º/åˆ é™¤/è¿ç§» | æ•°æ®å»¶è¿Ÿ < 2 ç§’ |
| âœ… **æ”¯æŒå¤§è§„æ¨¡** | å³ä½¿ 100k pods ä¹Ÿåªéœ€ 10MB å†…å­˜ | å¯æ‰©å±•æ€§å¼º |

#### å†…å­˜å ç”¨åˆ†æ

```
æ¯ä¸ª Pod å­˜å‚¨ï¼š
- UID: 36 bytes
- NodeName: ~20 bytes
- å…¶ä»–å¼€é”€: ~44 bytes
æ€»è®¡: ~100 bytes/pod

ä¸åŒè§„æ¨¡ä¸‹çš„å†…å­˜å ç”¨ï¼š
- 1,000 pods:   ~0.1 MB
- 10,000 pods:  ~1 MB
- 100,000 pods: ~10 MB

å¯¹æ¯”å®Œæ•´ Pod å¯¹è±¡ï¼š
- å®Œæ•´å¯¹è±¡: ~50 KB/pod
- è½»é‡çº§ç´¢å¼•: ~100 bytes/pod
å†…å­˜å‡å°‘: 500 å€ âœ…
```

#### åŠ£åŠ¿ä¸æƒè¡¡

| åŠ£åŠ¿ | å½±å“ | ç¼“è§£æªæ–½ |
|------|------|----------|
| âš ï¸ **å¯åŠ¨æ—¶å…¨é‡åŒæ­¥** | åˆæ¬¡åŒæ­¥éœ€è¦ 10-30 ç§’ | åå°å¼‚æ­¥åˆå§‹åŒ–ï¼Œä¸é˜»å¡æœåŠ¡ |
| âš ï¸ **å†…å­˜å ç”¨** | è¶…å¤§è§„æ¨¡é›†ç¾¤ï¼ˆ100k+ podsï¼‰å ç”¨ 10MB+ | ç›¸æ¯”å®Œæ•´å¯¹è±¡å·²å‡å°‘ 99.8% |
| âš ï¸ **Watch è¿æ¥æ–­å¼€** | ç½‘ç»œå¼‚å¸¸æ—¶å¯èƒ½ä¸¢å¤±äº‹ä»¶ | Informer è‡ªåŠ¨é‡è¿ + resync æœºåˆ¶ |

---

### æ–¹æ¡ˆ 2ï¼šä½¿ç”¨ Node å¯¹è±¡çš„ Pod åˆ†é…ä¿¡æ¯

#### æ ¸å¿ƒæ€è·¯
Node å¯¹è±¡çš„ `Status.Allocatable` å’Œ `Status.Capacity` å­—æ®µåŒ…å« Pod å®¹é‡ä¿¡æ¯ï¼Œä½†**ä¸åŒ…å«å½“å‰ Pod æ•°é‡**ã€‚

#### éªŒè¯ç»“æœ
âŒ **ä¸å¯è¡Œ** - Node å¯¹è±¡åªæä¾› Pod å®¹é‡ï¼ˆCapacity.Podsï¼‰ï¼Œä¸æä¾›å½“å‰ Pod æ•°é‡ã€‚

```go
// Node å¯¹è±¡ç¤ºä¾‹
node.Status.Capacity["pods"] = "110"       // æœ€å¤§ Pod æ•°
node.Status.Allocatable["pods"] = "110"    // å¯åˆ†é… Pod æ•°

// âŒ æ— æ³•è·å–å½“å‰ Pod æ•°é‡
```

---

### æ–¹æ¡ˆ 3ï¼šåˆ©ç”¨ Kubernetes Metrics API

#### æ ¸å¿ƒæ€è·¯
é€šè¿‡ metrics-server è·å– Pod ç›¸å…³æŒ‡æ ‡ã€‚

#### éªŒè¯ç»“æœ
âŒ **ä¸å¯è¡Œ** - Metrics API åªæä¾› CPU/å†…å­˜ä½¿ç”¨ç‡ï¼Œä¸æä¾› Pod æ•°é‡ã€‚

```bash
# Metrics API è¿”å›å†…å®¹
kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes/node-1

{
  "metadata": { "name": "node-1" },
  "usage": {
    "cpu": "2500m",
    "memory": "8Gi"
  }
}
# âŒ æ—  Pod æ•°é‡å­—æ®µ
```

---

### æ–¹æ¡ˆ 4ï¼šä½¿ç”¨ FieldSelector æŒ‰èŠ‚ç‚¹æŸ¥è¯¢

#### æ ¸å¿ƒæ€è·¯
ä¸ºæ¯ä¸ªèŠ‚ç‚¹å•ç‹¬æŸ¥è¯¢å…¶ Pod åˆ—è¡¨ã€‚

```go
func (s *Service) getNodePodCount(clusterName, nodeName string) (int, error) {
    ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
    defer cancel()
    
    // ä½¿ç”¨ FieldSelector åªæŸ¥è¯¢æŒ‡å®šèŠ‚ç‚¹çš„ Pod
    podList, err := client.CoreV1().Pods("").List(ctx, metav1.ListOptions{
        FieldSelector: fields.SelectorFromSet(fields.Set{
            "spec.nodeName": nodeName,
        }).String(),
    })
    
    if err != nil {
        return 0, err
    }
    
    // ç»Ÿè®¡éç»ˆæ­¢çŠ¶æ€çš„ Pod
    count := 0
    for _, pod := range podList.Items {
        if pod.Status.Phase != corev1.PodSucceeded && 
           pod.Status.Phase != corev1.PodFailed {
            count++
        }
    }
    
    return count, nil
}
```

#### ä¼˜åŠ¿
- âœ… ç²¾ç¡®æŸ¥è¯¢ï¼Œåªè·å–æŒ‡å®šèŠ‚ç‚¹çš„ Pod
- âœ… æ•°æ®é‡å°ï¼ˆç›¸æ¯”å…¨é‡æŸ¥è¯¢ï¼‰

#### åŠ£åŠ¿
- âŒ **éœ€è¦ N æ¬¡ API è°ƒç”¨**ï¼ˆN = èŠ‚ç‚¹æ•°ï¼‰
- âŒ 100 ä¸ªèŠ‚ç‚¹ = 100 æ¬¡ API è°ƒç”¨ = æ›´æ…¢
- âŒ å¯¹ API Server å‹åŠ›æ›´å¤§

#### ç»“è®º
âŒ **ä¸é€‚åˆæ‰¹é‡æŸ¥è¯¢åœºæ™¯**ï¼ˆè·å–æ‰€æœ‰èŠ‚ç‚¹çš„ Pod æ•°é‡ï¼‰

---

### æ–¹æ¡ˆ 5ï¼šä½¿ç”¨ Kubernetes Table API

#### æ ¸å¿ƒæ€è·¯
Table API å…è®¸è‡ªå®šä¹‰è¿”å›å­—æ®µï¼Œå‡å°‘æ•°æ®ä¼ è¾“é‡ã€‚

```go
// ä½¿ç”¨ Table API åªè·å– nodeName å’Œ phase
table, err := client.CoreV1().RESTClient().
    Get().
    Resource("pods").
    SetHeader("Accept", "application/json;as=Table;v=v1;g=meta.k8s.io").
    Do(ctx).
    Get()

// è§£æ Table ç»“æœ
for _, row := range table.Rows {
    nodeName := row.Cells[nodeNameIndex].(string)
    phase := row.Cells[phaseIndex].(string)
    // ç»Ÿè®¡
}
```

#### ä¼˜åŠ¿
- âœ… å‡å°‘æ•°æ®ä¼ è¾“é‡ï¼ˆåªè¿”å›å¿…è¦å­—æ®µï¼‰
- âœ… ä»ç„¶æ˜¯å•æ¬¡ API è°ƒç”¨

#### åŠ£åŠ¿
- âš ï¸ API ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦è§£æ Table æ ¼å¼
- âš ï¸ ä»ç„¶éœ€è¦éå†æ‰€æœ‰ Pod
- âš ï¸ æ— æ³•åˆ©ç”¨ç¼“å­˜ï¼ˆæ¯æ¬¡éƒ½æ˜¯å…¨é‡æŸ¥è¯¢ï¼‰

#### ç»“è®º
âš ï¸ **æœ‰ä¸€å®šä¼˜åŒ–æ•ˆæœï¼Œä½†ä¸å¦‚ Informer æ–¹æ¡ˆ**

---

### æ–¹æ¡ˆ 6ï¼šä½¿ç”¨ etcd ç›´æ¥æŸ¥è¯¢ï¼ˆä¸æ¨èï¼‰

#### æ ¸å¿ƒæ€è·¯
ç›´æ¥æŸ¥è¯¢ etcdï¼Œç»•è¿‡ API Serverã€‚

#### ç»“è®º
âŒ **å¼ºçƒˆä¸æ¨è**
- éœ€è¦ç›´æ¥è®¿é—® etcdï¼ˆå®‰å…¨é£é™©ï¼‰
- ç»•è¿‡ RBAC æƒé™æ§åˆ¶
- å¯èƒ½å¯¼è‡´æ•°æ®ä¸ä¸€è‡´
- è¿å Kubernetes æœ€ä½³å®è·µ

---

## æ¨èæ–¹æ¡ˆæ€»ç»“

### ğŸ† æœ€ä½³æ–¹æ¡ˆï¼šè½»é‡çº§ Pod Informer

#### ä¸ºä»€ä¹ˆæ¨èï¼Ÿ

1. **æ€§èƒ½æä½³**
   - æŸ¥è¯¢å“åº”: < 1msï¼ˆå†…å­˜æŸ¥è¯¢ï¼‰
   - æ•°æ®å®æ—¶æ€§: < 2 ç§’å»¶è¿Ÿ
   - é›¶ API è°ƒç”¨ï¼ˆæŸ¥è¯¢æ—¶ï¼‰

2. **å†…å­˜å¯æ§**
   - 10k pods â‰ˆ 1 MB
   - ç›¸æ¯”å®Œæ•´ Pod å¯¹è±¡å‡å°‘ **99.8%** å†…å­˜

3. **å®æ—¶å‡†ç¡®**
   - å¢é‡æ›´æ–°ï¼Œæ— éœ€å…¨é‡åˆ·æ–°
   - è‡ªåŠ¨è·Ÿè¸ª Pod åˆ›å»º/åˆ é™¤/è¿ç§»

4. **ä½ç»´æŠ¤æˆæœ¬**
   - Informer è‡ªåŠ¨å¤„ç†é‡è¿å’ŒåŒæ­¥
   - æ— éœ€é¢å¤–çš„ç¼“å­˜å¤±æ•ˆç­–ç•¥

#### ä¸ç°æœ‰æ–¹æ¡ˆå¯¹æ¯”

| ç»´åº¦ | ç°æœ‰æ–¹æ¡ˆï¼ˆåˆ†é¡µæŸ¥è¯¢+ç¼“å­˜ï¼‰ | è½»é‡çº§ Informer |
|------|-------------------------|----------------|
| **æŸ¥è¯¢å“åº”æ—¶é—´** | é¦–æ¬¡: 2-5ç§’<br>ç¼“å­˜: 200ms | < 1msï¼ˆå†…å­˜æŸ¥è¯¢ï¼‰ |
| **æ•°æ®å®æ—¶æ€§** | 5 åˆ†é’Ÿç¼“å­˜å»¶è¿Ÿ | < 2 ç§’ï¼ˆå®æ—¶ï¼‰ |
| **API è°ƒç”¨** | æ¯ 5 åˆ†é’Ÿä¸€æ¬¡å…¨é‡æŸ¥è¯¢ | ä»…å¯åŠ¨æ—¶åˆå§‹åŒ– |
| **å†…å­˜å ç”¨** | ç¼“å­˜ map: ~100KB | ~1MBï¼ˆ10k podsï¼‰ |
| **å‡†ç¡®æ€§** | ç¼“å­˜æœŸé—´å¯èƒ½ä¸å‡†ç¡® | å®æ—¶å‡†ç¡® |
| **å¤æ‚åº¦** | ä¸­ç­‰ | ä¸­ç­‰ |

---

## å®æ–½å»ºè®®

### é˜¶æ®µ 1ï¼šéªŒè¯å¯è¡Œæ€§ï¼ˆ1-2 å¤©ï¼‰

1. **åˆ›å»º PoC å®ç°**
   - å®ç° `PodCountCache` è½»é‡çº§ç¼“å­˜
   - é›†æˆåˆ°ç°æœ‰ Informer Service

2. **æ€§èƒ½æµ‹è¯•**
   - æµ‹è¯•ç¯å¢ƒï¼š100 èŠ‚ç‚¹ï¼Œ10k pods
   - ç›‘æ§å†…å­˜å ç”¨å’Œå“åº”æ—¶é—´

3. **å‹åŠ›æµ‹è¯•**
   - æ¨¡æ‹Ÿé«˜é¢‘ Pod åˆ›å»º/åˆ é™¤åœºæ™¯
   - éªŒè¯ Informer äº‹ä»¶å¤„ç†æ€§èƒ½

### é˜¶æ®µ 2ï¼šæ¸è¿›å¼éƒ¨ç½²ï¼ˆ3-5 å¤©ï¼‰

1. **åŒè½¨è¿è¡Œ**
   - åŒæ—¶è¿è¡Œ Informer å’Œç°æœ‰æŸ¥è¯¢
   - å¯¹æ¯”æ•°æ®å‡†ç¡®æ€§

2. **ç°åº¦åˆ‡æ¢**
   - éƒ¨åˆ†é›†ç¾¤ä½¿ç”¨ Informer æ–¹æ¡ˆ
   - è§‚å¯Ÿç¨³å®šæ€§å’Œæ€§èƒ½

3. **å…¨é‡åˆ‡æ¢**
   - ç¡®è®¤æ— é—®é¢˜åï¼Œå…¨éƒ¨åˆ‡æ¢åˆ° Informer
   - ä¿ç•™ç°æœ‰æŸ¥è¯¢ä½œä¸º fallback

### é˜¶æ®µ 3ï¼šä¼˜åŒ–å®Œå–„ï¼ˆé•¿æœŸï¼‰

1. **å†…å­˜ä¼˜åŒ–**
   - å¯¹è¶…å¤§è§„æ¨¡é›†ç¾¤ï¼ˆ100k+ podsï¼‰ï¼Œè€ƒè™‘ä½¿ç”¨ Redis å¤–éƒ¨å­˜å‚¨

2. **ç›‘æ§å‘Šè­¦**
   - æ·»åŠ  Informer å¥åº·æ£€æŸ¥
   - ç›‘æ§å†…å­˜å ç”¨å’Œäº‹ä»¶å¤„ç†å»¶è¿Ÿ

3. **é™çº§ç­–ç•¥**
   - Informer å¼‚å¸¸æ—¶è‡ªåŠ¨é™çº§åˆ°æŸ¥è¯¢æ–¹æ¡ˆ

---

## ä»£ç å®ç°ç¤ºä¾‹

### 1. è½»é‡çº§ Pod ç»Ÿè®¡ç¼“å­˜

```go
// backend/internal/podcache/pod_count_cache.go

package podcache

import (
    "sync"
    "sync/atomic"
    
    corev1 "k8s.io/api/core/v1"
    "kube-node-manager/internal/informer"
    "kube-node-manager/pkg/logger"
)

// PodCountCache è½»é‡çº§ Pod ç»Ÿè®¡ç¼“å­˜
type PodCountCache struct {
    // æ¯ä¸ªèŠ‚ç‚¹çš„ Pod è®¡æ•°: "cluster:node" -> int32
    nodePodCounts sync.Map
    
    // Pod ç´¢å¼•: "cluster:podUID" -> nodeName
    podToNode sync.Map
    
    logger *logger.Logger
}

// NewPodCountCache åˆ›å»º Pod ç»Ÿè®¡ç¼“å­˜
func NewPodCountCache(logger *logger.Logger) *PodCountCache {
    return &PodCountCache{
        logger: logger,
    }
}

// OnPodEvent å®ç° PodEventHandler æ¥å£
func (pc *PodCountCache) OnPodEvent(event informer.PodEvent) {
    // è¿‡æ»¤ç»ˆæ­¢çŠ¶æ€çš„ Pod
    if isTerminated(event.Pod.Status.Phase) {
        return
    }
    
    switch event.Type {
    case informer.EventTypeAdd:
        pc.handlePodAdd(event)
    case informer.EventTypeDelete:
        pc.handlePodDelete(event)
    case informer.EventTypeUpdate:
        pc.handlePodUpdate(event)
    }
}

// handlePodAdd å¤„ç† Pod æ·»åŠ äº‹ä»¶
func (pc *PodCountCache) handlePodAdd(event informer.PodEvent) {
    cluster := event.ClusterName
    podUID := string(event.Pod.UID)
    nodeName := event.Pod.Spec.NodeName
    
    if nodeName == "" {
        return // Pod å°šæœªè°ƒåº¦åˆ°èŠ‚ç‚¹
    }
    
    // é€’å¢èŠ‚ç‚¹ Pod è®¡æ•°
    pc.incrementPodCount(cluster, nodeName)
    
    // è®°å½• Pod åˆ°èŠ‚ç‚¹çš„æ˜ å°„
    pc.podToNode.Store(makeKey(cluster, podUID), nodeName)
}

// handlePodDelete å¤„ç† Pod åˆ é™¤äº‹ä»¶
func (pc *PodCountCache) handlePodDelete(event informer.PodEvent) {
    cluster := event.ClusterName
    podUID := string(event.Pod.UID)
    
    // è·å– Pod æ‰€åœ¨èŠ‚ç‚¹
    key := makeKey(cluster, podUID)
    if nodeNameInterface, ok := pc.podToNode.LoadAndDelete(key); ok {
        nodeName := nodeNameInterface.(string)
        pc.decrementPodCount(cluster, nodeName)
    }
}

// handlePodUpdate å¤„ç† Pod æ›´æ–°äº‹ä»¶
func (pc *PodCountCache) handlePodUpdate(event informer.PodEvent) {
    cluster := event.ClusterName
    podUID := string(event.Pod.UID)
    newNodeName := event.Pod.Spec.NodeName
    
    // æ£€æŸ¥ Pod æ˜¯å¦è¿ç§»åˆ°å…¶ä»–èŠ‚ç‚¹
    key := makeKey(cluster, podUID)
    if oldNodeInterface, ok := pc.podToNode.Load(key); ok {
        oldNodeName := oldNodeInterface.(string)
        
        if oldNodeName != newNodeName {
            // Pod è¿ç§»ï¼šæ—§èŠ‚ç‚¹ -1ï¼Œæ–°èŠ‚ç‚¹ +1
            pc.decrementPodCount(cluster, oldNodeName)
            pc.incrementPodCount(cluster, newNodeName)
            pc.podToNode.Store(key, newNodeName)
        }
    } else {
        // æ–° Podï¼ˆå¯èƒ½ä» Pending å˜ä¸º Runningï¼‰
        if newNodeName != "" {
            pc.incrementPodCount(cluster, newNodeName)
            pc.podToNode.Store(key, newNodeName)
        }
    }
    
    // æ£€æŸ¥ Pod æ˜¯å¦å˜ä¸ºç»ˆæ­¢çŠ¶æ€
    if isTerminated(event.Pod.Status.Phase) {
        pc.handlePodDelete(event)
    }
}

// GetNodePodCount è·å–èŠ‚ç‚¹çš„ Pod æ•°é‡
func (pc *PodCountCache) GetNodePodCount(cluster, nodeName string) int {
    key := makeKey(cluster, nodeName)
    if countInterface, ok := pc.nodePodCounts.Load(key); ok {
        count := countInterface.(*int32)
        return int(atomic.LoadInt32(count))
    }
    return 0
}

// GetAllNodePodCounts è·å–æ‰€æœ‰èŠ‚ç‚¹çš„ Pod æ•°é‡
func (pc *PodCountCache) GetAllNodePodCounts(cluster string) map[string]int {
    result := make(map[string]int)
    
    prefix := cluster + ":"
    pc.nodePodCounts.Range(func(key, value interface{}) bool {
        keyStr := key.(string)
        if len(keyStr) > len(prefix) && keyStr[:len(prefix)] == prefix {
            nodeName := keyStr[len(prefix):]
            count := value.(*int32)
            result[nodeName] = int(atomic.LoadInt32(count))
        }
        return true
    })
    
    return result
}

// incrementPodCount é€’å¢èŠ‚ç‚¹ Pod è®¡æ•°
func (pc *PodCountCache) incrementPodCount(cluster, nodeName string) {
    key := makeKey(cluster, nodeName)
    
    countInterface, _ := pc.nodePodCounts.LoadOrStore(key, new(int32))
    count := countInterface.(*int32)
    atomic.AddInt32(count, 1)
}

// decrementPodCount é€’å‡èŠ‚ç‚¹ Pod è®¡æ•°
func (pc *PodCountCache) decrementPodCount(cluster, nodeName string) {
    key := makeKey(cluster, nodeName)
    
    if countInterface, ok := pc.nodePodCounts.Load(key); ok {
        count := countInterface.(*int32)
        newCount := atomic.AddInt32(count, -1)
        
        // å¦‚æœè®¡æ•°é™ä¸º 0ï¼Œå¯ä»¥é€‰æ‹©åˆ é™¤é”®ï¼ˆèŠ‚çœå†…å­˜ï¼‰
        if newCount <= 0 {
            pc.nodePodCounts.Delete(key)
        }
    }
}

// è¾…åŠ©å‡½æ•°
func makeKey(cluster, identifier string) string {
    return cluster + ":" + identifier
}

func isTerminated(phase corev1.PodPhase) bool {
    return phase == corev1.PodSucceeded || phase == corev1.PodFailed
}
```

### 2. æ‰©å±• Informer Service æ”¯æŒ Pod

```go
// backend/internal/informer/informer.go

// PodEvent Pod äº‹ä»¶ç±»å‹
type PodEvent struct {
    Type        EventType
    ClusterName string
    Pod         *corev1.Pod
    OldPod      *corev1.Pod
    Timestamp   time.Time
}

// PodEventHandler Pod äº‹ä»¶å¤„ç†å™¨æ¥å£
type PodEventHandler interface {
    OnPodEvent(event PodEvent)
}

// Service æ‰©å±•
type Service struct {
    // ... ç°æœ‰å­—æ®µ ...
    podHandlers []PodEventHandler // Pod äº‹ä»¶å¤„ç†å™¨åˆ—è¡¨
}

// RegisterPodHandler æ³¨å†Œ Pod äº‹ä»¶å¤„ç†å™¨
func (s *Service) RegisterPodHandler(handler PodEventHandler) {
    s.mu.Lock()
    defer s.mu.Unlock()
    s.podHandlers = append(s.podHandlers, handler)
    s.logger.Infof("Registered pod event handler: %T", handler)
}

// StartPodInformer å¯åŠ¨ Pod Informer
func (s *Service) StartPodInformer(clusterName string, clientset *kubernetes.Clientset) error {
    s.mu.Lock()
    defer s.mu.Unlock()
    
    if _, exists := s.informers[clusterName]; !exists {
        return fmt.Errorf("node informer not started for cluster %s", clusterName)
    }
    
    factory := s.informers[clusterName]
    
    // è·å– PodInformer
    podInformer := factory.Core().V1().Pods().Informer()
    
    // æ³¨å†Œäº‹ä»¶å¤„ç†å™¨
    podInformer.AddEventHandler(cache.ResourceEventHandlerFuncs{
        AddFunc: func(obj interface{}) {
            pod := obj.(*corev1.Pod)
            s.handlePodAdd(clusterName, pod)
        },
        UpdateFunc: func(oldObj, newObj interface{}) {
            oldPod := oldObj.(*corev1.Pod)
            newPod := newObj.(*corev1.Pod)
            s.handlePodUpdate(clusterName, oldPod, newPod)
        },
        DeleteFunc: func(obj interface{}) {
            pod := obj.(*corev1.Pod)
            s.handlePodDelete(clusterName, pod)
        },
    })
    
    s.logger.Infof("Successfully started Pod Informer for cluster: %s", clusterName)
    return nil
}

// äº‹ä»¶å¤„ç†æ–¹æ³•
func (s *Service) handlePodAdd(clusterName string, pod *corev1.Pod) {
    event := PodEvent{
        Type:        EventTypeAdd,
        ClusterName: clusterName,
        Pod:         pod,
        Timestamp:   time.Now(),
    }
    s.notifyPodHandlers(event)
}

func (s *Service) handlePodUpdate(clusterName string, oldPod, newPod *corev1.Pod) {
    event := PodEvent{
        Type:        EventTypeUpdate,
        ClusterName: clusterName,
        Pod:         newPod,
        OldPod:      oldPod,
        Timestamp:   time.Now(),
    }
    s.notifyPodHandlers(event)
}

func (s *Service) handlePodDelete(clusterName string, pod *corev1.Pod) {
    event := PodEvent{
        Type:        EventTypeDelete,
        ClusterName: clusterName,
        Pod:         pod,
        Timestamp:   time.Now(),
    }
    s.notifyPodHandlers(event)
}

func (s *Service) notifyPodHandlers(event PodEvent) {
    s.mu.RLock()
    handlers := make([]PodEventHandler, len(s.podHandlers))
    copy(handlers, s.podHandlers)
    s.mu.RUnlock()
    
    for _, handler := range handlers {
        go func(h PodEventHandler) {
            defer func() {
                if r := recover(); r != nil {
                    s.logger.Errorf("Pod event handler panic: %v", r)
                }
            }()
            h.OnPodEvent(event)
        }(handler)
    }
}
```

### 3. é›†æˆåˆ° K8s Service

```go
// backend/internal/service/k8s/k8s.go

// Service æ‰©å±•å­—æ®µ
type Service struct {
    // ... ç°æœ‰å­—æ®µ ...
    podCountCache *podcache.PodCountCache
}

// åˆå§‹åŒ–æ—¶æ³¨å†Œ Pod äº‹ä»¶å¤„ç†å™¨
func NewService(logger *logger.Logger, cache *cache.K8sCache, 
                informerSvc *informer.Service) *Service {
    s := &Service{
        // ... ç°æœ‰åˆå§‹åŒ– ...
        podCountCache: podcache.NewPodCountCache(logger),
    }
    
    // æ³¨å†Œ Pod äº‹ä»¶å¤„ç†å™¨
    if informerSvc != nil {
        informerSvc.RegisterPodHandler(s.podCountCache)
    }
    
    return s
}

// enrichNodesWithMetrics ä¿®æ”¹ä¸ºä½¿ç”¨ Pod Informer ç¼“å­˜
func (s *Service) enrichNodesWithMetrics(clusterName string, nodes []NodeInfo) {
    // ... CPU/å†…å­˜æŒ‡æ ‡è·å–ï¼ˆä¿æŒä¸å˜ï¼‰...
    
    // ä¼˜åŒ–ï¼šç›´æ¥ä» Pod Informer ç¼“å­˜è·å– Pod æ•°é‡
    // å¦‚æœ Informer å°šæœªå°±ç»ªï¼Œé™çº§åˆ°æŸ¥è¯¢æ–¹æ¡ˆ
    podCounts := s.getPodCountsFromInformerOrFallback(clusterName, nodeNames)
    
    // ... åç»­å¤„ç†ï¼ˆä¿æŒä¸å˜ï¼‰...
}

// getPodCountsFromInformerOrFallback ä¼˜å…ˆä½¿ç”¨ Informerï¼Œå¤±è´¥æ—¶é™çº§åˆ°æŸ¥è¯¢
func (s *Service) getPodCountsFromInformerOrFallback(clusterName string, 
                                                     nodeNames []string) map[string]int {
    // å°è¯•ä» Pod Informer ç¼“å­˜è·å–
    if s.podCountCache != nil {
        podCounts := s.podCountCache.GetAllNodePodCounts(clusterName)
        if len(podCounts) > 0 {
            s.logger.Debugf("Got pod counts from Informer cache for cluster %s", clusterName)
            return podCounts
        }
    }
    
    // é™çº§ï¼šä½¿ç”¨ç°æœ‰çš„æŸ¥è¯¢ + ç¼“å­˜æ–¹æ¡ˆ
    s.logger.Debugf("Falling back to API query for pod counts: cluster=%s", clusterName)
    fetchFunc := func() map[string]int {
        return s.getNodesPodCounts(clusterName, nodeNames)
    }
    return s.cache.GetPodCounts(clusterName, nodeNames, fetchFunc)
}
```

---

## é¢„æœŸæ•ˆæœ

### æ€§èƒ½æå‡

| æŒ‡æ ‡ | å½“å‰æ–¹æ¡ˆ | Informer æ–¹æ¡ˆ | æ”¹å–„ |
|------|---------|--------------|------|
| **é¦–æ¬¡æŸ¥è¯¢** | 2-5 ç§’ï¼ˆç¼“å­˜æœªå‘½ä¸­ï¼‰ | < 1ms | âš¡ **99.9% â†“** |
| **åç»­æŸ¥è¯¢** | 200msï¼ˆç¼“å­˜å‘½ä¸­ï¼‰ | < 1ms | âš¡ **99.5% â†“** |
| **æ•°æ®å®æ—¶æ€§** | 5 åˆ†é’Ÿå»¶è¿Ÿ | < 2 ç§’ | âœ… **å®æ—¶** |
| **API è°ƒç”¨é¢‘ç‡** | æ¯ 5 åˆ†é’Ÿä¸€æ¬¡ | ä»…å¯åŠ¨æ—¶ | âœ… **é™ä½ 99%** |
| **å†…å­˜å ç”¨** | ~100KBï¼ˆç¼“å­˜ï¼‰ | ~1MBï¼ˆ10k podsï¼‰ | âš ï¸ **å¢åŠ  10 å€** |

### ç»¼åˆè¯„ä»·

âœ… **å¼ºçƒˆæ¨èå®æ–½** - æ€§èƒ½æå‡å·¨å¤§ï¼Œå†…å­˜å¢åŠ å¯æ§ï¼Œå®æ—¶æ€§æ˜¾è‘—æ”¹å–„ã€‚

---

## æ€»ç»“

### æœ€ä½³å®è·µ

1. **ä¼˜å…ˆä½¿ç”¨ Informer** - å¯¹äºå˜åŒ–é¢‘ç‡é€‚ä¸­ã€æ•°é‡å¯æ§çš„èµ„æºï¼ˆå¦‚ Nodeã€Pod è®¡æ•°ï¼‰
2. **è½»é‡çº§å­˜å‚¨** - åªç¼“å­˜å¿…è¦ä¿¡æ¯ï¼Œä¸ç¼“å­˜å®Œæ•´å¯¹è±¡
3. **é™çº§ç­–ç•¥** - Informer å¼‚å¸¸æ—¶è‡ªåŠ¨é™çº§åˆ° API æŸ¥è¯¢
4. **ç›‘æ§å‘Šè­¦** - ç›‘æ§ Informer å¥åº·çŠ¶æ€å’Œå†…å­˜å ç”¨

### ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. âœ… **è¯„å®¡æ–¹æ¡ˆ** - ä¸å›¢é˜Ÿè®¨è®ºå¹¶ç¡®è®¤å®æ–½
2. âœ… **PoC å¼€å‘** - åˆ›å»ºåŸå‹éªŒè¯å¯è¡Œæ€§  
3. âœ… **ä»£ç å®æ–½** - å®Œæ•´å®ç°å·²å®Œæˆï¼ˆv2.24.0ï¼‰
4. ğŸš§ **æ€§èƒ½æµ‹è¯•** - åœ¨æµ‹è¯•ç¯å¢ƒéªŒè¯æ•ˆæœ
5. ğŸš§ **ç°åº¦éƒ¨ç½²** - æ¸è¿›å¼ä¸Šçº¿åˆ°ç”Ÿäº§ç¯å¢ƒ

---

## å®æ–½çŠ¶æ€ï¼ˆv2.24.0ï¼‰

### âœ… å·²å®Œæˆ

1. **æ ¸å¿ƒå®ç°**
   - âœ… `backend/internal/podcache/pod_count_cache.go` - è½»é‡çº§Podç»Ÿè®¡ç¼“å­˜
   - âœ… `backend/internal/informer/informer.go` - Pod Informeræ”¯æŒ
   - âœ… `backend/internal/service/k8s/k8s.go` - é›†æˆå’Œé™çº§ç­–ç•¥
   - âœ… `backend/internal/realtime/manager.go` - å¯åŠ¨Pod Informer
   - âœ… `backend/internal/service/services.go` - æ³¨å†ŒPodEventHandler

2. **å…³é”®ç‰¹æ€§**
   - âœ… è½»é‡çº§å†…å­˜å­˜å‚¨ï¼ˆ100 bytes/podï¼‰
   - âœ… å®æ—¶ç»Ÿè®¡ï¼ˆå¢é‡æ›´æ–°ï¼‰
   - âœ… é™çº§ç­–ç•¥ï¼ˆè‡ªåŠ¨fallbackåˆ°åˆ†é¡µæŸ¥è¯¢ï¼‰
   - âœ… å¼‚æ­¥å¯åŠ¨ï¼ˆä¸é˜»å¡ç³»ç»Ÿåˆå§‹åŒ–ï¼‰
   - âœ… å®Œå–„çš„é”™è¯¯å¤„ç†

3. **éƒ¨ç½²å‹å¥½**
   - âœ… å‘åå…¼å®¹ï¼ˆPod Informerå¤±è´¥æ—¶è‡ªåŠ¨é™çº§ï¼‰
   - âœ… é›¶é…ç½®ï¼ˆè‡ªåŠ¨å¯ç”¨ï¼‰
   - âœ… å¹³æ»‘å‡çº§ï¼ˆæ— éœ€æ•°æ®è¿ç§»ï¼‰

### ğŸ§ª å¾…æµ‹è¯•

1. **åŠŸèƒ½æµ‹è¯•**
   - éªŒè¯Podç»Ÿè®¡å‡†ç¡®æ€§
   - æµ‹è¯•Podè¿ç§»åœºæ™¯
   - éªŒè¯é™çº§ç­–ç•¥

2. **æ€§èƒ½æµ‹è¯•**
   - æµ‹è¯•ä¸åŒè§„æ¨¡é›†ç¾¤ï¼ˆ1kã€10kã€100k podsï¼‰
   - æµ‹é‡å“åº”æ—¶é—´å’Œå†…å­˜å ç”¨
   - å‹åŠ›æµ‹è¯•ï¼ˆé«˜é¢‘Podåˆ›å»º/åˆ é™¤ï¼‰

3. **ç¨³å®šæ€§æµ‹è¯•**
   - é•¿æ—¶é—´è¿è¡Œç¨³å®šæ€§
   - Informeré‡è¿æµ‹è¯•
   - å¼‚å¸¸åœºæ™¯æµ‹è¯•

### ä½¿ç”¨æ–¹å¼

**æ— éœ€ä»»ä½•é…ç½®ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨ï¼š**

1. åœ¨é›†ç¾¤æ³¨å†Œæ—¶å¯åŠ¨Pod Informer
2. å®æ—¶ç»Ÿè®¡Podæ•°é‡
3. æŸ¥è¯¢æ—¶ä¼˜å…ˆä½¿ç”¨Informerç¼“å­˜
4. Informeræœªå°±ç»ªæ—¶è‡ªåŠ¨é™çº§åˆ°åˆ†é¡µæŸ¥è¯¢

**æ—¥å¿—è¾“å‡ºï¼š**

```log
INFO: Registered Pod event handler: *podcache.PodCountCache
INFO: Successfully started Pod Informer for cluster: jobsscz-k8s-cluster
DEBUG: Using Pod Informer cache for cluster jobsscz-k8s-cluster (fast path)
```

**é™çº§æ—¥å¿—ï¼š**

```log
WARNING: Failed to start Pod Informer for cluster xxx: ...
INFO: Pod count will fall back to API query mode
DEBUG: Pod Informer not ready for cluster xxx, falling back to paginated query
```

---

**å‚è€ƒæ–‡æ¡£**:
- [Resource Management Strategy](./resource-management-strategy.md)
- [Large Cluster Timeout Optimization](./large-cluster-timeout-optimization.md)
- [Kubernetes Informer å®˜æ–¹æ–‡æ¡£](https://kubernetes.io/docs/reference/using-api/api-concepts/#efficient-detection-of-changes)

